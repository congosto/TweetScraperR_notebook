---
title: "twitter scraping"
output:
  html_document:
    df_print: paged
---

## TweetScraperR_notebook.Rmd

### Estructura de los datos

```         
root ----+----data-+---- dataset_1
         |         |
         |         +---- dataset_n
         |
         +---notebooks-+---- utils
                       |
                       +---- TweetScraperR_notebook.Rmd
                       |
                       +---- TweetScraperR_notebook_charts.Rmd
```

El **dataset** es el directorio donde se almacenan los datos. Las capturas se distinguen por su prefijo. En un dataset puede haber ficheros con distintos prefijos, todo depende de cómo se organice el trabajo. Para cada prefijo se generan dos ficheros: uno con los datos básicos de los tweets y otro con los metadatos.

### TweetScraperR_notebook.Rmd

En este cuaderno se han adaptado las librerías **TweetScraperR** a la forma de trabajar con t_hoarder_R.

**Funcionalidades**:

1.  Se especifica el rango de fechas inicial y final de la captura y la frecuencia de la descarga. La frecuencia se debe ajustar para que el número de tweets que se obtienen en cada petición no supere los 600 mensajes.
2.  Los datos se almacenan en formato csv
3.  Guarda contexto de la descarga. Si se interrumpe, se reanudará en el punto que lo dejó

**Tipos de descargas**:

1.  **Get Tweets Historical Search**: descarga una consulta en un periodo definido con una frecuencia establecida
2.  **Get Tweets Historical Timeline**: descarga el timeline de una lista de usuarios en un periodo definido con una frecuencia establecida. No se obtienen los RTs porque equivale a una consulta del tipo from:usuario
3.  **Get Tweets images**: obtiene las imágenes de una descarga ya realizada

### Cómo usar el cuaderno

1.  Cuando se abre el cuaderno, ejecutar los chunks: **Init notebook**, **Import libraries**, **Import functions**
2.  En el chunk de **Open session** rellenar el usuario y password para la descarga y ejecutarlo.
3.  Elegir el tipo de descarga, rellenar los parámetros que se encuentran en el principio del chunk y ejecutarlo. Se pueden ejecutar distintos tipos de descargas sin necesidad de volver ejecutar el **Open session**.
4.  Si ocurre un error durante la descarga, se recomienda hacer un reset a R y empezar en el punto 1. La descarga se reanudará en el punto que se dejó.
5.  Antes de salir ejecutar **close session**.

### Código

#### Init notebook

```{r setup, 	echo = FALSE,message = FALSE,	warning = FALSE,include=FALSE, cache = FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
require("knitr")
## setting working directory
opts_knit$set(root.dir = "./")
```

#### Import libraries

```{r library}
if(!"tidyverse" %in% installed.packages()) {install.packages("tidyverse")}
if(!"devtools" %in% installed.packages()) {install.packages("devtools")}
if(!"TweetScraperR" %in% installed.packages()) {devtools::install_github("agusnieto77/TweetScraperR")}
library("tidyverse")
library("TweetScraperR")
library("glue")
```

#### Import functions

```{r funtions, include=FALSE}
source("utils/format_meta_tweets.R")   # Funciones de formateo de mensajes
source("utils/context.R")   # Funciones contexto de la descarga

```

#### Open session

```{r start-sesion}
data_path = "../data"
if(!file.exists(data_path)) {
 dir.create(img_path)
}
assign("USER", "rest_018",  envir = .GlobalEnv)
assign("PASS", "milikk",  envir = .GlobalEnv)
closeTwitter()
Sys.sleep(5)
openTwitter()
```



#### Get Tweets Timeline

Descarga el timeline de un usuario

```{r Get-Tweets-Timeline, message=TRUE, warning=TRUE}
dataset <- "congosto"
prefix <- "congosto_1"
sleep_time <- 300

output <- file.path(data_path,dataset)
output_file <- file.path(output,paste0(prefix,".csv"))

list_users = c(
"congosto",
"t_hoarder"
)

flag_append <- FALSE
flag_head <- TRUE
users_downloaded <- list()
dates_downloaded <- list()
context <- get_context_user(output, prefix)
if (!is.null (context)){
  flag_append <- TRUE
  flag_head <- FALSE
  users_downloaded <- context$username
  dates_downloaded <- context$last_date
}
for (i_user in 1:(length(list_users))){
  user <- list_users[[i_user]]
  if (is.element(user,users_downloaded)){
    print(glue("--> {user} already downloaded"))
  }else{
    openTimeline(user)
    print(glue("--> download user {user}"))
    tweets <- getTweetsTimeline (
      username = user,
      n_tweets = 5000,
      xuser = Sys.getenv(USER),
      xpass = Sys.getenv(PASS),
      dir = NULL,
      save = FALSE  
    ) 
    if (!is.null(tweets)){
      tweets <- tweets %>%
        mutate (tweet = str_replace_all (tweet, '[\n\r]+',' '))
      write_csv (tweets, output_file, append = flag_append, col_names = flag_head)
      flag_append <- TRUE
      flag_head <- FALSE
    }
    put_context_user(output, max(tweets$fecha), i_user, user)
    if (i_user < length(list_users)) {  # No esperar después de la última iteración
      Sys.sleep(3)
      glue("Esperando {sleep_time} segundos antes de la próxima iteración...\n")
      Sys.sleep(sleep_time-3)
    }
  }
}
# Ordenar y quitar repetidos
print ("Removing duplicates and sorting by user/date")
output_order_file <- file.path(output,paste0(prefix,"_order.csv"))
tweets <- read_csv(output_file, show_col_types = FALSE) %>%
  group_by(url)  %>%
  slice (1) %>%
  arrange(user,fecha)
write_csv (tweets, output_order_file)

```

#### close session

```{r close-session}
closeTwitter()

```
