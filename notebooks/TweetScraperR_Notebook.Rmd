---
title: "twitter scraping"
output:
  html_document:
    df_print: paged
---

## TweetScraperR_notebook.Rmd

### Estructura de los datos

```         
root ----+----data-+---- dataset_1
         |         |
         |         +---- dataset_n
         |
         +---notebooks-+---- utils
                       |
                       +---- TweetScraperR_notebook.Rmd
                       |
                       +---- TweetScraperR_notebook_charts.Rmd
```

El **dataset** es el directorio donde se almacenan los datos. Las capturas se distinguen por su prefijo. En un dataset puede haber ficheros con distintos prefijos, todo depende de cómo se organice el trabajo. Para cada prefijo se generan dos ficheros: uno con los datos básicos de los tweets y otro con los metadatos.

### TweetScraperR_notebook.Rmd

En este cuaderno se han adaptado las librerías **TweetScraperR** a la forma de trabajar con t_hoarder_R.

**Funcionalidades**:

1.  Se especifica el rango de fechas inicial y final de la captura y la frecuencia de la descarga. La frecuencia se debe ajustar para que el número de tweets que se obtienen en cada petición no supere los 600 mensajes.
2.  Los datos se almacenan en formato csv
3.  Guarda contexto de la descarga. Si se interrumpe, se reanudará en el punto que lo dejó

**Tipos de descargas**:

1.  **Get Tweets Historical Search**: descarga una consulta en un periodo definido con una frecuencia establecida
2.  **Get Tweets Historical Timeline**: descarga el timeline de una lista de usuarios en un periodo definido con una frecuencia establecida. No se obtienen los RTs porque equivale a una consulta del tipo from:usuario
3.  **Get Tweets images**: obtiene las imágenes de una descarga ya realizada

### Cómo usar el cuaderno

1.  Cuando se abre el cuaderno, ejecutar los chunks: **Init notebook**, **Import libraries**, **Import functions**
2.  En el chunk de **Open session** rellenar el usuario y password para la descarga y ejecutarlo.
3.  Elegir el tipo de descarga, rellenar los parámetros que se encuentran en el principio del chunk y ejecutarlo. Se pueden ejecutar distintos tipos de descargas sin necesidad de volver ejecutar el **Open session**.
4.  Si ocurre un error durante la descarga, se recomienda hacer un reset a R y empezar en el punto 1. La descarga se reanudará en el punto que se dejó.
5.  Antes de salir ejecutar **close session**.

### Código

#### Init notebook

```{r setup, 	echo = FALSE,message = FALSE,	warning = FALSE,include=FALSE, cache = FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
require("knitr")
## setting working directory
opts_knit$set(root.dir = "./")
```

#### Import libraries

```{r library}
if(!"tidyverse" %in% installed.packages()) {install.packages("tidyverse")}
if(!"devtools" %in% installed.packages()) {install.packages("devtools")}
if(!"TweetScraperR" %in% installed.packages()) {devtools::install_github("agusnieto77/TweetScraperR")}
library("tidyverse")
library("TweetScraperR")
```

#### Import functions

```{r funtions, include=FALSE}
source("utils/format_meta_tweets.R")   # Funciones de formateo de mensajes
source("utils/context.R")   # Funciones contexto de la descarga

```

#### Open session

```{r start-sesion}
data_path = "../data"
if(!file.exists(data_path)) {
 dir.create(img_path)
}
assign("USER", "xxxxx",  envir = .GlobalEnv)
assign("PASS", "xxxxx",  envir = .GlobalEnv)
openTwitter()
```

#### Get Tweets Historical Search

```{r Get-Tweets-Historical-Search, message=TRUE, warning=TRUE}
data_path = "../data"
dataset <- "trump"
prefix <- "trump"
query <- "trump"
since <- "2025-02-12 00:00:01"
until <- "2025-02-18 00:00:01"
frecuency <- "8 hours"

output <- file.path(data_path,dataset)
output_file <- file.path(output,paste0(prefix,".csv"))
output_meta_file <- file.path(output,paste0(prefix,"_meta.csv"))

flag_append <- FALSE
flag_head <- TRUE
last_date <- get_context_search(output,prefix)
if (!is.null (last_date)){
  flag_append <- TRUE
  flag_head <- FALSE
  since <- last_date
}

date_sequence <- seq(as.POSIXct(since), as.POSIXct(until), by=frecuency)
if (length(date_sequence) > 1){
  for (i in 1:(length(date_sequence) - 1))  {
    ini_date = paste0(gsub(" ", "_", date_sequence[[i]]),"_UTC")
    end_date = paste0(gsub(" ", "_", date_sequence[[i+1]]),"_UTC")
    print(paste(ini_date, end_date))
    tweets <- getTweetsHistoricalSearch (
      search = query,
      timeout = 10,
      n_tweets =5000,
      since = ini_date,
      until = end_date,
      xuser = Sys.getenv(USER),
      xpass = Sys.getenv(PASS),
      dir = NULL,
      save = FALSE  
    ) 
    if (!is.null(tweets)){
      meta_tweets <- extractTweetsData(tweets)
      tweets <- tweets %>%
      select(- art_html) %>%
      mutate (tweet = str_replace_all (tweet, '[\n\r]+',' '))
      write_csv (tweets, output_file, append = flag_append, col_names = flag_head)
      meta_tweets <- format_meta_tweets(meta_tweets) 
      write_csv (meta_tweets, output_meta_file, append = flag_append, col_names = flag_head)
      put_context_search(output, prefix, date_sequence[[i+1]])
      flag_append <- TRUE
      flag_head <- FALSE
    }
    Sys.sleep(360)
  }
}
# Ordenar y quitar repetidos
print ("Removing duplicates and sorting by user/date")
output_order_file <- file.path(output,paste0(prefix,"_order.csv"))
output_meta_order_file <- file.path(output,paste0(prefix,"_meta_order.csv"))
tweets <- read_csv(output_file, show_col_types = FALSE) %>%
  group_by(url)  %>%
  slice (1) %>%
  arrange(fecha)
write_csv (tweets, output_order_file)
tweets_meta <- read_csv(output_meta_file, show_col_types = FALSE) %>%
  group_by(url)  %>%
  slice (1) %>%
  arrange(fecha)
write_csv (tweets_meta, output_meta_order_file)

```

#### Get Tweets Historical Timeline

La query que realiza es from:user since:xxx-xx-xx until:xxxx-xx-xx

OJO!!! no descarga RTs realizados por el usuario, solo tweets propios

```{r Get-Tweets-Historical-Timeline, message=TRUE, warning=TRUE}
data_path = "../data"
dataset <- "congosto"
prefix <- "congosto"
since <- "2025-01-25 00:00:01"
until <- "2025-02-18 00:00:01"
frecuency <- "1 day"

output <- file.path(data_path,dataset)
output_file <- file.path(output,paste0(prefix,".csv"))
output_meta_file <- file.path(output,paste0(prefix,"_meta.csv"))

list_users = c(
"congosto",
"t_hoarder"
)

flag_append <- FALSE
flag_head <- TRUE
context <- get_context_user(output, prefix)
if (!is.null (context)){
  flag_append <- TRUE
  flag_head <- FALSE
  users_downloaded <- context$username
  dates_downloaded <- context$last_date
}
for (i_user in 1:(length(list_users))){
  since_partial <- since
  if (!is.null (context)){
    if (i_user <= length(users_downloaded)){
      if (list_users[[i_user]] == users_downloaded[[i_user]]){
        since_partial <- dates_downloaded[[i_user]]
      }
    }
  }
  user <- list_users[[i_user]]
  print(paste("--> download user", user))
  date_sequence <- seq(as.POSIXct(since_partial), as.POSIXct(until), by=frecuency)
  if (length(date_sequence) > 1){
    for (i_date in 1:(length(date_sequence) - 1))  {
      ini_date = paste0(gsub(" ", "_", date_sequence[[i_date]]),"_UTC")
      end_date = paste0(gsub(" ", "_", date_sequence[[i_date+1]]),"_UTC")
      print(paste(ini_date, end_date))
      tweets <- getTweetsHistoricalTimeline (
        username = user,
        timeout = 10,
        n_tweets = 5000,
        since = ini_date,
        until = end_date,
        xuser = Sys.getenv(USER),
        xpass = Sys.getenv(PASS),
        dir = NULL,
        save = FALSE  
      ) 
      if (!is.null(tweets)){
        meta_tweets <- extractTweetsData(tweets)
        tweets <- tweets %>%
        select(- art_html) %>%
        mutate (tweet = str_replace_all (tweet, '[\n\r]+',' '))
        write_csv (tweets, output_file, append = flag_append, col_names = flag_head)
        meta_tweets <- format_meta_tweets(meta_tweets) 
        write_csv (meta_tweets, output_meta_file, append = flag_append, col_names = flag_head)
        flag_append <- TRUE
        flag_head <- FALSE
      }
      put_context_user(output, date_sequence[[i_date+1]],i_user, user)
    }
  }
}
# Ordenar y quitar repetidos
print ("Removing duplicates and sorting by user/date")
output_order_file <- file.path(output,paste0(prefix,"_order.csv"))
output_meta_order_file <- file.path(output,paste0(prefix,"_meta_order.csv"))
tweets <- read_csv(output_file, show_col_types = FALSE) %>%
  group_by(url)  %>%
  slice (1) %>%
  arrange(user,fecha)
write_csv (tweets, output_order_file)
tweets_meta <- read_csv(output_meta_file, show_col_types = FALSE) %>%
  group_by(url)  %>%
  slice (1) %>%
  arrange(username,fecha)
write_csv (tweets_meta, output_meta_order_file)



```

#### Get Tweets images

```{r Get-Tweets-images, message=TRUE, warning=TRUE}
data_path = "../data"
dataset = "trump"
prefix = "trump"
file_in <- file.path(data_path,dataset,paste0(prefix,"_meta.csv"))
output <- file.path(data_path,dataset,paste0(prefix,"_images"))
context <- file.path(data_path,dataset,paste0(prefix,"_context_images"))
output_context <- file.path(context,"context_images.csv")
if(!file.exists(output)) {
 dir.create(output)
 print (paste("created directory",output))
}
# obtenemos el contexto
if(!file.exists(context)) {
 dir.create(context)
 print (paste("created directory",context))
 writeLines("downloaded", output_context)
 list_downloaded <- list()
}else{
  list_downloaded <- read_csv(output_context)
  list_downloaded <- list_downloaded$downloaded
}
tweets <- read_csv(file_in) %>%
  filter (!is.na(links_img_post))
# group 100 by 100
list_images <- unique(tweets$links_img_post)
group <- get_grupos(list_images, 100)
for (i in seq_along(group)) {
  print (paste("group ",i))
  group_images <- as.list(group[[i]])
  group_images <- setdiff(group_images, list_downloaded)
  if (length(group_images) > 0){
    print( paste("Descargando", length(group_images)))
    getTweetsImages(
      urls = group_images,
      directorio = output)
    # save context
    for (image in list_images) {
      cat(image, file = output_context, sep = "\n", append = TRUE)
    }
  }
}
```

#### close session

```{r close-session}
closeTwitter()

```
